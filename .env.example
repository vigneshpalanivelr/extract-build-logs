# GitLab Pipeline Log Extraction System Configuration

# ============================================================================
# REQUIRED SETTINGS
# ============================================================================

# GitLab instance URL (no trailing slash)
# Example: https://gitlab.com or https://gitlab.company.com
GITLAB_URL=https://gitlab.com

# GitLab Personal Access Token with api scope
# Create at: GitLab → Profile → Access Tokens → Select 'api' scope
GITLAB_TOKEN=your_gitlab_token_here

# OPTIONAL: Base64 encode the GitLab token for obfuscation
# Set GITLAB_TOKEN_ENCODING=base64 to decode the token
# Example: GITLAB_TOKEN_ENCODING=base64
# To encode: echo -n 'your_token' | base64
# GITLAB_TOKEN_ENCODING=plain

# Docker image name (required)
DOCKER_IMAGE_NAME=bfa-gitlab-pipeline-extractor

# Docker container name (required)
DOCKER_CONTAINER_NAME=bfa-gitlab-pipeline-extractor

# Host directory for Docker volume mount (required)
# This is the host path that gets mounted to the container
DOCKER_LOGS_DIR=./logs

# Port for webhook listener server (required)
WEBHOOK_PORT=8000

# Logging level (required)
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Directory where logs will be stored inside the container (required)
LOG_OUTPUT_DIR=./logs

# Number of retry attempts for failed API calls (required)
RETRY_ATTEMPTS=3

# Delay in seconds between retry attempts (required)
RETRY_DELAY=60


# ============================================================================
# OPTIONAL SETTINGS
# ============================================================================

# Secret token for webhook validation (recommended for production)
# This should match the secret configured in GitLab webhook settings
# Leave empty to disable validation (not recommended for production)
WEBHOOK_SECRET=

# Number of days to retain logs before cleanup
# Used by scripts/cleanup_old_logs.sh to remove old logs
# Default: 90
LOG_RETENTION_DAYS=90


# ============================================================================
# LOG FILTERING CONFIGURATION
# ============================================================================

# Which pipeline statuses to save logs for
# Options: all, failed, success, running, canceled, skipped
# Multiple values: failed,canceled,skipped
# Default: all (saves logs from all pipelines)
LOG_SAVE_PIPELINE_STATUS=failed

# Which projects to save logs for (comma-separated project IDs)
# Leave empty to save all projects
# Example: 123,456,789
# Default: (empty = all projects)
LOG_SAVE_PROJECTS=

# Which projects to exclude from logging (comma-separated project IDs)
# Only used if LOG_SAVE_PROJECTS is empty
# Example: 999,888
# Default: (empty = no exclusions)
LOG_EXCLUDE_PROJECTS=

# Which job statuses to save logs for within a pipeline
# Options: all, failed, success, canceled, skipped
# Multiple values: failed,canceled
# Default: all (saves logs from all jobs)
LOG_SAVE_JOB_STATUS=failed

# Save pipeline metadata even if no logs are saved
# Useful for tracking all pipelines even if only storing failed logs
# Options: true, false
# Default: true
LOG_SAVE_METADATA_ALWAYS=true


# ============================================================================
# API POSTING CONFIGURATION
# ============================================================================

# Enable API posting for pipeline logs (instead of or in addition to file storage)
# NOTE: When enabled, requires BFA_HOST to be set
# API endpoint is auto-constructed as: http://BFA_HOST:8000/api/analyze
# Authentication: If BFA_SECRET_KEY is set, it will be used as Bearer token (optional)
# Options: true, false
# Default: false
API_POST_ENABLED=true

# Request timeout in seconds for API calls
# Default: 30
API_POST_TIMEOUT=180

# Enable retry logic for failed API requests
# Uses the same retry configuration as GitLab API calls (RETRY_ATTEMPTS, RETRY_DELAY)
# Options: true, false
# Default: true
API_POST_RETRY_ENABLED=true

# Also save logs to files when API posting is enabled
# Options: true, false
# Default: false
# - false: API only (file storage as fallback if API fails)
# - true: Dual mode (always save to both API and files)
API_POST_SAVE_TO_FILE=false


# ============================================================================
# JENKINS INTEGRATION CONFIGURATION
# ============================================================================

# SINGLE INSTANCE vs MULTIPLE INSTANCES:
# ------------------------------------------------------------------------------
# There are two ways to configure Jenkins integration:
#
# 1. SINGLE INSTANCE (using .env file - this file):
#    - Configure the settings below for one Jenkins instance
#    - Simple setup for organizations with one Jenkins server
#    - Backward compatible with existing configurations
#
# 2. MULTIPLE INSTANCES (using jenkins_instances.json file):
#    - Support multiple Jenkins instances with different credentials
#    - Create jenkins_instances.json from jenkins_instances.json.example
#    - Each instance can have different URL, credentials, and webhook secrets
#    - Jenkinsfile must include jenkins_url in webhook payload
#    - If jenkins_instances.json exists, it takes precedence over .env
#
# See jenkins_instances.json.example and DOCUMENTATION.md for multi-instance setup
# ------------------------------------------------------------------------------

# Enable Jenkins webhook support
# Options: true, false
# Default: false
JENKINS_ENABLED=false

# Jenkins instance URL (no trailing slash)
# Required if JENKINS_ENABLED=true (for single instance setup)
# Example: https://jenkins.example.com
# Note: For multiple Jenkins instances, use jenkins_instances.json instead
JENKINS_URL=

# Jenkins username for API authentication
# Required if JENKINS_ENABLED=true (for single instance setup)
# Note: For multiple Jenkins instances, use jenkins_instances.json instead
JENKINS_USER=

# Jenkins API token for authentication
# Required if JENKINS_ENABLED=true (for single instance setup)
# Create at: Jenkins → User → Configure → API Token → Add new Token
# Note: For multiple Jenkins instances, use jenkins_instances.json instead
JENKINS_API_TOKEN=

# OPTIONAL: Base64 encode the Jenkins API token for obfuscation
# Set JENKINS_API_TOKEN_ENCODING=base64 to decode the token
# To encode: echo -n 'your_token' | base64
# JENKINS_API_TOKEN_ENCODING=plain

# Secret token for Jenkins webhook validation (optional)
# This should match the token sent in X-Jenkins-Token header from Jenkinsfile
# Leave empty to disable validation
# Note: For multiple Jenkins instances, each can have its own secret in jenkins_instances.json
JENKINS_WEBHOOK_SECRET=

# OPTIONAL: Base64 encode the Jenkins webhook secret for obfuscation
# Set JENKINS_WEBHOOK_SECRET_ENCODING=base64 to decode the secret
# To encode: echo -n 'your_secret' | base64
# JENKINS_WEBHOOK_SECRET_ENCODING=plain

# Filter out handled failures from Jenkins pipeline processing
# When enabled, analyzes STEPS within failed stages to identify real failures
# Filters out step failures that were caught with try-catch/fallback mechanisms
#
# The Problem:
# - Jenkins pipelines use try-catch with fallback mechanisms
# - A stage may have multiple steps, some fail but are caught (handled)
# - Blue Ocean marks ALL failed steps, including handled ones
# - Only the LAST failed step that stopped the stage is the real failure
#
# How it works:
# - For each failed stage, examines individual steps (stageFlowNodes)
# - If a step fails but subsequent steps execute successfully, it was "handled"
# - Only reports the last failed step with no successful steps after it
#
# Example:
#   Stage "Build" (FAILED):
#     Step 1: compile (FAILED) ← caught by try-catch, ignored
#     Step 2: fallback compile (SUCCESS)
#     Step 3: tests (FAILED) ← caught, ignored
#     Step 4: more scripts (SUCCESS)
#     Step 5: final build (FAILED) ← REAL FAILURE, reported
#
# Options: true, false
# Default: true (recommended - only report real failures)
# Set to false to report ALL step failures including handled ones
JENKINS_FILTER_HANDLED_FAILURES=true


# ============================================================================
# BFA JWT TOKEN GENERATION CONFIGURATION
# ============================================================================

# BFA Host - Hostname/IP of BFA server (without http:// prefix)
# This will be used to construct the API endpoint: http://BFA_HOST:8000/api/analyze
# Example: bfa-server.example.com or 192.168.1.100
BFA_HOST=

# Secret key for BFA JWT token signing (for /api/token endpoint)
# NOTE: BFA_SECRET_KEY is completely separate from GITLAB_TOKEN
#       - BFA_SECRET_KEY: Used for signing JWT tokens for API authentication
#       - GITLAB_TOKEN: Used for authenticating with GitLab API
# REQUIRED: This must be set explicitly - no fallback to GITLAB_TOKEN
# If not set, JWT token generation will be disabled and errors will be logged
#
# How to obtain BFA_SECRET_KEY:
# Option 1: Get from BFA server
#   curl -X POST http://BFA_HOST:8000/api/token
#   Response: {"token": "<TOKEN>"}
#   Then set: BFA_SECRET_KEY=<TOKEN>
#
# Option 2: Generate manually
#   python -c "import secrets; print(secrets.token_urlsafe(32))"
BFA_SECRET_KEY=

# OPTIONAL: Base64 encode the BFA secret key for obfuscation
# Set BFA_SECRET_KEY_ENCODING=base64 to decode the key
# To encode: echo -n 'your_secret_key' | base64
# BFA_SECRET_KEY_ENCODING=plain


# ============================================================================
# LOG ERROR EXTRACTION CONFIGURATION
# ============================================================================

# Number of log lines to include BEFORE each error line
# This provides context to help LLM understand what led to the error
# Default: 50
# Range: 0-1000 (higher values = more context, larger payloads)
# Example: If error is at line 500, includes lines 450-500
ERROR_CONTEXT_LINES_BEFORE=50

# Number of log lines to include AFTER each error line
# This provides context to see immediate consequences of the error
# Default: 10
# Range: 0-100
# Example: If error is at line 500, includes lines 500-510
ERROR_CONTEXT_LINES_AFTER=10

# Patterns to IGNORE when detecting errors (comma-separated, case-insensitive)
# Lines matching these patterns will NOT be considered errors even if they
# match ERROR_PATTERNS (like 'error', 'failed', etc.)
# Use this to filter out false positives
#
# Example use cases:
#   - "error: tag" - ignore docker tag messages containing "error:"
#   - "failed to create" - ignore specific benign failure messages
#   - "0 errors" - ignore success messages containing "error" word
#
# Default: (empty = no ignore patterns)
# Example: error: tag,0 errors,warning treated as error
ERROR_IGNORE_PATTERNS=

# Maximum number of log lines to process per build (safety limit)
# Prevents memory issues with extremely large build logs
# Default: 100000
# Range: 10000-10000000 (10k to 10 million)
# Note: Very large values may cause memory issues
MAX_LOG_LINES=100000

# Number of tail lines to try fetching first (hybrid fetch strategy)
# The system first tries to fetch only the last N lines (fast)
# If errors are found, uses tail only; otherwise fetches full log
# Default: 5000
# Range: 1000-50000
TAIL_LOG_LINES=5000

# Streaming chunk size in bytes when downloading large logs
# Larger chunks = faster download but more memory usage
# Default: 8192
# Range: 1024-65536 (1KB to 64KB)
STREAM_CHUNK_SIZE=8192


# ============================================================================
# USAGE INSTRUCTIONS
# ============================================================================
#
# 1. Copy this file to .env:
#    cp .env.example .env
#
# 2. Edit .env and set your GitLab URL and token
#
# 3. (Optional) Configure webhook secret in both .env and GitLab
#
# 4. Build and run the Docker container:
#    ./manage_container.py build
#    ./manage_container.py start
#
#    Or run directly (development):
#    python src/webhook_listener.py
#
# 5. View configuration:
#    ./manage_container.py config
#
# For more information, see README.md and OPERATIONS.md
#
# ============================================================================
